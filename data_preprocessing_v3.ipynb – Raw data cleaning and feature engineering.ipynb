{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPuOUGlNU8RoGT/ciL4PvAN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/orevs-com/Stock-Market-Price-Prediction/blob/main/data_preprocessing_v3.ipynb%20%E2%80%93%20Raw%20data%20cleaning%20and%20feature%20engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8zfClC4RtzD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enhanced Stock Price Prediction Using LSTM with Technical Indicators: A Comparative Study with XGBoost and ARIMA\n",
        "\n",
        "# by Orevaoghene Otiede"
      ],
      "metadata": {
        "id": "WC1ufaXavhsS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the required libraries for the project\n"
      ],
      "metadata": {
        "id": "ltjuMkQJv6Gx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall numpy pmdarima -y\n",
        "!pip install numpy==1.26.4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "G2ng91O0KGbH",
        "outputId": "47e55ec9-7ddc-4ca3-e400-d0ae529feb6d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n",
            "Found existing installation: pmdarima 2.0.4\n",
            "Uninstalling pmdarima-2.0.4:\n",
            "  Successfully uninstalled pmdarima-2.0.4\n",
            "Collecting numpy==1.26.4\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Installing collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "56336f172aab4ad2906f7f394f591f79"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install pandas_ta\n",
        "!pip install pandas_ta statsmodels xgboost\n",
        "!pip install pmdarima"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtdIHgN4PIOc",
        "outputId": "d1125cde-7555-4a55-9271-ed2984f6125f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas_ta in /usr/local/lib/python3.11/dist-packages (0.3.14b0)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.11/dist-packages (0.14.4)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from pandas_ta) (2.2.2)\n",
            "Requirement already satisfied: numpy<3,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels) (1.26.4)\n",
            "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /usr/local/lib/python3.11/dist-packages (from statsmodels) (1.15.3)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels) (1.0.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels) (24.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->pandas_ta) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->pandas_ta) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->pandas_ta) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->pandas_ta) (1.17.0)\n",
            "Collecting pmdarima\n",
            "  Using cached pmdarima-2.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (1.5.1)\n",
            "Requirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (1.15.3)\n",
            "Requirement already satisfied: statsmodels>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (0.14.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (2.4.0)\n",
            "Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (75.2.0)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19->pmdarima) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19->pmdarima) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19->pmdarima) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22->pmdarima) (3.6.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.13.2->pmdarima) (1.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=0.19->pmdarima) (1.17.0)\n",
            "Using cached pmdarima-2.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (2.2 MB)\n",
            "Installing collected packages: pmdarima\n",
            "Successfully installed pmdarima-2.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Corrcting the numpy import for pandas_ta\n",
        "path = \"/usr/local/lib/python3.11/dist-packages/pandas_ta/momentum/squeeze_pro.py\"\n",
        "\n",
        "# Read the file and replace the line\n",
        "with open(path, 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Replace the incorrect import\n",
        "content = content.replace(\"from numpy import NaN as npNaN\", \"from numpy import nan as npNaN\")\n",
        "\n",
        "# Write the updated content back\n",
        "with open(path, 'w') as file:\n",
        "    file.write(content)\n",
        "\n",
        "print(\"File updated successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JTkWLG1PjRE",
        "outputId": "6223f71f-43fc-4ac2-8d0e-7f24ee59cb26"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File updated successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing the Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pandas_ta as ta\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn import linear_model\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
        "import keras.backend as K\n",
        "from pmdarima import auto_arima\n",
        "import yfinance as yf\n",
        "import os\n",
        "from statsmodels.tsa.arima.model import ARIMA as PyARIMA\n",
        "import warnings\n",
        "import pickle\n",
        "import xgboost as xgb"
      ],
      "metadata": {
        "id": "YxI-S3pDwFZV"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the drive module from the google.colab package.\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount the Google Drive to the specified mount point in the Colab environment.\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "G1O2owTDPbuS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09208405-d9fd-4c16-bda3-25011c7980f5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "List of ticker symbols for Training Companies"
      ],
      "metadata": {
        "id": "UENSEN76WMjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of Ticker Symbols\n",
        "train_tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'NVDA', 'META']\n",
        "test_tickers = ['TSLA', 'AVGO', 'COST', 'NFLX', 'ADBE', 'INTC']"
      ],
      "metadata": {
        "id": "-XEBSgMHWqas"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading and Saving the Stock Market Price Data"
      ],
      "metadata": {
        "id": "XzQ0BF2Vycvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the date range\n",
        "start_date = \"2015-01-01\"\n",
        "end_date = \"2024-12-31\"\n",
        "# Define the file path\n",
        "drive_path = '/content/drive/MyDrive/Colab Notebooks/Stock Market Data/'\n",
        "\n",
        "# Function to download and save data for a list of tickers\n",
        "def download_and_save_tickers(tickers, start, end, base_path):\n",
        "    for ticker in tickers:\n",
        "        try:\n",
        "            # Dwonload data from Yahoo Finance with auto adjusted closed price\n",
        "            df = yf.download(ticker, start=start, end=end, interval=\"1d\",\n",
        "                             auto_adjust=True)\n",
        "\n",
        "            if not df.empty:\n",
        "                file_path = os.path.join(base_path, f\"{ticker}.csv\")\n",
        "                df.to_csv(file_path)\n",
        "                print(f\"Successfully downloaded and saved data for {ticker}\")\n",
        "            else:\n",
        "                print(f\"No data found for {ticker} in the specified date range.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading data for {ticker}: {e}\")\n",
        "\n",
        "# Download and save training data\n",
        "download_and_save_tickers(train_tickers, start_date, end_date, drive_path)\n",
        "\n",
        "# Download and save testing data\n",
        "download_and_save_tickers(test_tickers, start_date, end_date, drive_path)\n",
        "\n",
        "print(\"\\nAll data download attempts complete.\")"
      ],
      "metadata": {
        "id": "XhurJAlLzrCC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04a05242-4ab3-4baf-c9ea-41d6833c04d5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully downloaded and saved data for AAPL\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully downloaded and saved data for MSFT\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully downloaded and saved data for GOOGL\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully downloaded and saved data for AMZN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully downloaded and saved data for NVDA\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully downloaded and saved data for META\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully downloaded and saved data for TSLA\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully downloaded and saved data for AVGO\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully downloaded and saved data for COST\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully downloaded and saved data for NFLX\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully downloaded and saved data for ADBE\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully downloaded and saved data for INTC\n",
            "\n",
            "All data download attempts complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating for Technical Indicators"
      ],
      "metadata": {
        "id": "31sCJm6tPdiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the directory where the stock data CSV files are saved\n",
        "data_dir = '/content/drive/MyDrive/Colab Notebooks/Stock Market Data/'\n",
        "output_dir = '/content/drive/MyDrive/Colab Notebooks/Data with Indicators/'\n",
        "\n",
        "# Create an Output Directory\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Get a list of all CSV files\n",
        "csv_files = [f for f in os.listdir(data_dir) if f.endswith('.csv')]\n",
        "\n",
        "for file_name in csv_files:\n",
        "    ticker = file_name.replace('.csv', '') # Extract ticker from file name\n",
        "    file_path = os.path.join(data_dir, file_name)\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(file_path, skiprows=3) # Considering Multi Index Rows\n",
        "        df.columns = ['Date', 'Close', 'High', 'Low', 'Open', 'Volume']\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "        df.set_index('Date', inplace=True)\n",
        "        print(f\"Calculating indicators for {ticker}...\")\n",
        "\n",
        "        if not all(col in df.columns for col in ['Open', 'High', 'Low', 'Volume']):\n",
        "            print(f\"Skipping {ticker}: Missing OHLCV columns after parsing.\")\n",
        "            continue\n",
        "\n",
        "        # Calculating Technical Indicators using pandas_ta\n",
        "\n",
        "        # Trend Indicators\n",
        "        df.ta.sma(length=200, append=True)\n",
        "        df.ta.sma(length=50, append=True)\n",
        "        df.ta.ema(length=26, append=True)\n",
        "        df.ta.macd(append=True)\n",
        "\n",
        "        # Momentum Indicators\n",
        "        df.ta.rsi(length=14, append=True)\n",
        "        df.ta.willr(append=True)\n",
        "\n",
        "        # Volatility Indicator\n",
        "        df.ta.bbands(append=True)\n",
        "\n",
        "        # Volume Indicators\n",
        "        df.ta.obv(append=True)\n",
        "        df.ta.cmf(append=True)\n",
        "\n",
        "        # Adding Daily Returns and Log Returns\n",
        "        df['Daily_Return'] = df['Close'].pct_change()\n",
        "        df['Log_Return'] = np.log(df['Close'] / df['Close'].shift(1))\n",
        "        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "        # Add Lagged Features for all calculated indicators\n",
        "\n",
        "        # Columns from SMA (Trend Indicators)\n",
        "        lag_cols = ['SMA_200', 'SMA_50', 'EMA_26']\n",
        "\n",
        "        # Columns from MACD (Trend Indicators)\n",
        "        lag_cols.extend(['MACD_12_26_9', 'MACDh_12_26_9', 'MACDs_12_26_9'])\n",
        "\n",
        "        # Columns from RSI, Williams (Momentum Indicators)\n",
        "        lag_cols.extend(['RSI_14', 'WILLR_14'])\n",
        "\n",
        "        # Columns from Bollinger Bands (Volatiity Indicators)\n",
        "        lag_cols.extend(['BBL_5_2.0', 'BBM_5_2.0', 'BBU_5_2.0', 'BBB_5_2.0', 'BBP_5_2.0'])\n",
        "\n",
        "        # Columns from OBV, CMF (Volume Indicators)\n",
        "        lag_cols.extend(['OBV', 'CMF_20'])\n",
        "\n",
        "        # Columns from Daily Returns and Log Returns\n",
        "        lag_cols.extend(['Daily_Return', 'Log_Return'])\n",
        "\n",
        "        # Apply lagging\n",
        "        for col in lag_cols:\n",
        "            if col in df.columns:\n",
        "                df[f'Lag_{col}'] = df[col].shift(1)\n",
        "            else:\n",
        "                print(f\"  Column '{col}' not found for {ticker} .\")\n",
        "\n",
        "         # Data Cleaning after adding Technical Indicators and Lagged Features\n",
        "        rows = len(df)\n",
        "        df.dropna(inplace=True)\n",
        "        print(f\"  Dropped {rows - len(df)} rows with NaN values for {ticker}.\")\n",
        "\n",
        "        # Save the processed DataFrame to a new CSV\n",
        "        output_file_path = os.path.join(output_dir, f\"{ticker}_indicators.csv\")\n",
        "        df.to_csv(output_file_path)\n",
        "        print(f\"Saved Technical Indicators for {ticker} to {output_file_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {ticker}: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uZV2wnrPncq",
        "outputId": "e5c30d00-23ac-4efe-adf4-8e1a0b29395f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating indicators for GOOGL...\n",
            "  Dropped 200 rows with NaN values for GOOGL.\n",
            "Saved Technical Indicators for GOOGL to /content/drive/MyDrive/Colab Notebooks/Data with Indicators/GOOGL_indicators.csv\n",
            "Calculating indicators for AMZN...\n",
            "  Dropped 200 rows with NaN values for AMZN.\n",
            "Saved Technical Indicators for AMZN to /content/drive/MyDrive/Colab Notebooks/Data with Indicators/AMZN_indicators.csv\n",
            "Calculating indicators for NVDA...\n",
            "  Dropped 200 rows with NaN values for NVDA.\n",
            "Saved Technical Indicators for NVDA to /content/drive/MyDrive/Colab Notebooks/Data with Indicators/NVDA_indicators.csv\n",
            "Calculating indicators for AVGO...\n",
            "  Dropped 200 rows with NaN values for AVGO.\n",
            "Saved Technical Indicators for AVGO to /content/drive/MyDrive/Colab Notebooks/Data with Indicators/AVGO_indicators.csv\n",
            "Calculating indicators for META...\n",
            "  Dropped 200 rows with NaN values for META.\n",
            "Saved Technical Indicators for META to /content/drive/MyDrive/Colab Notebooks/Data with Indicators/META_indicators.csv\n",
            "Calculating indicators for AAPL...\n",
            "  Dropped 200 rows with NaN values for AAPL.\n",
            "Saved Technical Indicators for AAPL to /content/drive/MyDrive/Colab Notebooks/Data with Indicators/AAPL_indicators.csv\n",
            "Calculating indicators for MSFT...\n",
            "  Dropped 200 rows with NaN values for MSFT.\n",
            "Saved Technical Indicators for MSFT to /content/drive/MyDrive/Colab Notebooks/Data with Indicators/MSFT_indicators.csv\n",
            "Calculating indicators for COST...\n",
            "  Dropped 200 rows with NaN values for COST.\n",
            "Saved Technical Indicators for COST to /content/drive/MyDrive/Colab Notebooks/Data with Indicators/COST_indicators.csv\n",
            "Calculating indicators for TSLA...\n",
            "  Dropped 200 rows with NaN values for TSLA.\n",
            "Saved Technical Indicators for TSLA to /content/drive/MyDrive/Colab Notebooks/Data with Indicators/TSLA_indicators.csv\n",
            "Calculating indicators for ADBE...\n",
            "  Dropped 200 rows with NaN values for ADBE.\n",
            "Saved Technical Indicators for ADBE to /content/drive/MyDrive/Colab Notebooks/Data with Indicators/ADBE_indicators.csv\n",
            "Calculating indicators for INTC...\n",
            "  Dropped 200 rows with NaN values for INTC.\n",
            "Saved Technical Indicators for INTC to /content/drive/MyDrive/Colab Notebooks/Data with Indicators/INTC_indicators.csv\n",
            "Calculating indicators for NFLX...\n",
            "  Dropped 200 rows with NaN values for NFLX.\n",
            "Saved Technical Indicators for NFLX to /content/drive/MyDrive/Colab Notebooks/Data with Indicators/NFLX_indicators.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing"
      ],
      "metadata": {
        "id": "vTHqBIFCfdAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppress warnings from pmdarima\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Defining the directory where the stock data CSV files are saved\n",
        "output_dir = '/content/drive/MyDrive/Colab Notebooks/Data with Indicators/'\n",
        "processed_dir = '/content/drive/MyDrive/Colab Notebooks/Processed Data/'\n",
        "\n",
        "# Create an Output Directory\n",
        "os.makedirs(processed_dir, exist_ok=True)\n",
        "\n",
        "# Structure for Data Preparation\n",
        "TARGET_COLUMN = 'Close' # Next day's Adjusted Close\n",
        "SPLIT_DATE = '2023-01-01' # Date to split training and testing data\n",
        "LSTM_LOOKBACK = 60 # Number of past days to look at for LSTM sequences\n",
        "\n",
        "# Define ticker for feature selection\n",
        "FEATURE_SELECTION_TICKER = 'AAPL'\n",
        "\n",
        "# Dictionary to store prepared data for each model and each stock\n",
        "all_prepared_data = {\n",
        "    'lstm': {},\n",
        "    'xgboost': {},\n",
        "    'arima': {}\n",
        "}\n",
        "\n",
        "# Get all processed CSV files\n",
        "files = [f for f in os.listdir(output_dir) if f.endswith('_indicators.csv')]\n",
        "\n",
        "# Feature selection to determine best Indicators\n",
        "\n",
        "# Load data for selected feature selection ticker\n",
        "fs_path = os.path.join(output_dir, f\"{FEATURE_SELECTION_TICKER}_indicators.csv\")\n",
        "if not os.path.exists(fs_path):\n",
        "    print(f\"Feature selection ticker {FEATURE_SELECTION_TICKER}_indicators.csv not found\")\n",
        "\n",
        "fs_df = pd.read_csv(fs_path, index_col='Date', parse_dates=True)\n",
        "\n",
        "# Define Target and Features for feature selection model\n",
        "fs_df['Target'] = fs_df[TARGET_COLUMN].shift(-1)\n",
        "fs_exclude_cols = ['Close', 'Target']\n",
        "fs_feature_cols = [col for col in fs_df.columns if pd.api.types.is_numeric_dtype(fs_df[col]) and col not in fs_exclude_cols]\n",
        "\n",
        "fs_X_df = fs_df[fs_feature_cols]\n",
        "fs_y_df = fs_df['Target']\n",
        "\n",
        "# Clean NaNs for feature selection data\n",
        "fs_combined_df = pd.concat([fs_X_df, fs_y_df], axis=1).dropna()\n",
        "fs_X_clean = fs_combined_df[fs_feature_cols]\n",
        "fs_y_clean = fs_combined_df['Target']\n",
        "\n",
        "if fs_X_clean.empty or fs_y_clean.empty:\n",
        "    print(f\"Not enough data for {FEATURE_SELECTION_TICKER} for feature selection\")\n",
        "\n",
        "# Split data for feature selection model\n",
        "fs_X_train = fs_X_clean[fs_X_clean.index < SPLIT_DATE]\n",
        "fs_y_train = fs_y_clean[fs_y_clean.index < SPLIT_DATE]\n",
        "\n",
        "if fs_X_train.empty or fs_y_train.empty:\n",
        "    print(f\"Training data for {FEATURE_SELECTION_TICKER} empty.\")\n",
        "\n",
        "# Scale features for XGBoost using MinMaxScaler\n",
        "fs_scaler_X = MinMaxScaler(feature_range=(0, 1))\n",
        "fs_X_train_scaled = fs_scaler_X.fit_transform(fs_X_train)\n",
        "fs_X_train_scaled_df = pd.DataFrame(fs_X_train_scaled, columns=fs_X_train.columns, index=fs_X_train.index)\n",
        "\n",
        "\n",
        "# Train a simple XGBoost Regressor for feature importance\n",
        "print(f\" Training temporary XGBoost model on {FEATURE_SELECTION_TICKER} to get feature importances...\")\n",
        "model_xgb_fs = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "model_xgb_fs.fit(fs_X_train_scaled_df, fs_y_train)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = model_xgb_fs.feature_importances_\n",
        "features_df = pd.DataFrame({'Feature': fs_X_train_scaled_df.columns, 'Importance': feature_importances})\n",
        "features_df = features_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"\\n  Top 10 Features by Importance:\")\n",
        "print(features_df.head(10))\n",
        "\n",
        "# Select the top 3 features\n",
        "selected_features = features_df['Feature'].head(3).tolist()\n",
        "print(f\"\\n  Selected Top 3 Features for all models: {selected_features}\")\n",
        "\n",
        "# Data Preparation using selected features\n",
        "\n",
        "for file_name in files:\n",
        "    ticker = file_name.replace('_indicators.csv', '')\n",
        "    file_path = os.path.join(output_dir, file_name)\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(file_path, index_col='Date', parse_dates=True)\n",
        "\n",
        "        # Define Target Variable (Y Axis)\n",
        "        df['Target'] = df[TARGET_COLUMN].shift(-1)\n",
        "\n",
        "        # Feature Selection ( X Axis)\n",
        "        three_selected_features = [f for f in selected_features if f in df.columns and pd.api.types.is_numeric_dtype(df[f])]\n",
        "\n",
        "        if len(three_selected_features) < 3:\n",
        "            print(f\"Not all top 3 features found in {ticker}. Using {len(three_selected_features)} features.\")\n",
        "            if not three_selected_features:\n",
        "                 print(f\"Skipping {ticker}: No selected features found.\")\n",
        "                 continue\n",
        "\n",
        "\n",
        "        X_df = df[three_selected_features]\n",
        "        y_df = df['Target']\n",
        "\n",
        "        # Dropping any other NaNs\n",
        "        combined_df = pd.concat([X_df, y_df], axis=1).dropna()\n",
        "        X_clean = combined_df[three_selected_features]\n",
        "        y_clean = combined_df['Target']\n",
        "\n",
        "        if X_clean.empty or y_clean.empty:\n",
        "            print(f\"Skipping {ticker}: Not enough clean data after NaN removal. (Shape X: {X_clean.shape}, Y: {y_clean.shape})\")\n",
        "            continue\n",
        "        print(f\"Cleaned data shape: X={X_clean.shape}, y={y_clean.shape}\")\n",
        "\n",
        "        # Train-Test Split\n",
        "        X_train = X_clean[X_clean.index < SPLIT_DATE]\n",
        "        X_test = X_clean[X_clean.index >= SPLIT_DATE]\n",
        "        y_train = y_clean[y_clean.index < SPLIT_DATE]\n",
        "        y_test = y_clean[y_clean.index >= SPLIT_DATE]\n",
        "\n",
        "        if X_train.empty or X_test.empty or y_train.empty or y_test.empty:\n",
        "            print(f\"Skipping {ticker}: Train or test set is empty.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Train/Test split: Train X={X_train.shape}, y={y_train.shape} | Test X={X_test.shape}, y={y_test.shape}\")\n",
        "\n",
        "        # Feature Scaling\n",
        "        scaler_X = MinMaxScaler(feature_range=(0, 1))\n",
        "        X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "        X_test_scaled = scaler_X.transform(X_test)\n",
        "\n",
        "        scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
        "        y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1))\n",
        "        y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1))\n",
        "\n",
        "        # Model-Specific Data Preparation\n",
        "\n",
        "        # LSTM Data Preparation\n",
        "        train_generator = TimeseriesGenerator(X_train_scaled, y_train_scaled.flatten(),\n",
        "                                              length=LSTM_LOOKBACK, batch_size=1)\n",
        "        test_generator = TimeseriesGenerator(X_test_scaled, y_test_scaled.flatten(),\n",
        "                                             length=LSTM_LOOKBACK, batch_size=1)\n",
        "\n",
        "        X_train_lstm = np.array([train_generator[i][0][0] for i in range(len(train_generator))])\n",
        "        y_train_lstm = np.array([train_generator[i][1] for i in range(len(train_generator))])\n",
        "        X_test_lstm = np.array([test_generator[i][0][0] for i in range(len(test_generator))])\n",
        "        y_test_lstm = np.array([test_generator[i][1] for i in range(len(test_generator))])\n",
        "\n",
        "        X_train_lstm = X_train_lstm.reshape(X_train_lstm.shape[0], LSTM_LOOKBACK, X_train_scaled.shape[1])\n",
        "        X_test_lstm = X_test_lstm.reshape(X_test_lstm.shape[0], LSTM_LOOKBACK, X_test_scaled.shape[1])\n",
        "\n",
        "        print(f\"    LSTM Train X shape: {X_train_lstm.shape}, y shape: {y_train_lstm.shape}\")\n",
        "        print(f\"    LSTM Test X shape: {X_test_lstm.shape}, y shape: {y_test_lstm.shape}\")\n",
        "\n",
        "        all_prepared_data['lstm'][ticker] = {\n",
        "            'X_train': X_train_lstm, 'y_train': y_train_lstm,\n",
        "            'X_test': X_test_lstm, 'y_test': y_test_lstm,\n",
        "            'scaler_X': scaler_X, 'scaler_y': scaler_y\n",
        "        }\n",
        "\n",
        "        # XGBoost Data Preparation\n",
        "        X_train_xgb = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
        "        X_test_xgb = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
        "\n",
        "        y_train_xgb = y_train\n",
        "        y_test_xgb = y_test\n",
        "\n",
        "        print(f\"    XGBoost Train X shape: {X_train_xgb.shape}, y shape: {y_train_xgb.shape}\")\n",
        "        print(f\"    XGBoost Test X shape: {X_test_xgb.shape}, y shape: {y_test_xgb.shape}\")\n",
        "\n",
        "        all_prepared_data['xgboost'][ticker] = {\n",
        "            'X_train': X_train_xgb, 'y_train': y_train_xgb,\n",
        "            'X_test': X_test_xgb, 'y_test': y_test_xgb,\n",
        "            'scaler_X': scaler_X\n",
        "        }\n",
        "\n",
        "        # ARIMA Data Preparation\n",
        "        arima_series_train = y_train.copy()\n",
        "        arima_series_test = y_test.copy()\n",
        "\n",
        "        arima_order = None\n",
        "        try:\n",
        "            stepwise_fit = auto_arima(arima_series_train, start_p=1, start_q=1,\n",
        "                                      max_p=5, max_q=5, m=1,\n",
        "                                      d=None, seasonal=False,\n",
        "                                      trace=False, error_action='ignore', suppress_warnings=True,\n",
        "                                      stepwise=True)\n",
        "            arima_order = stepwise_fit.order\n",
        "            print(f\"    Auto-ARIMA found order (p,d,q): {arima_order}\")\n",
        "        except Exception as arima_e:\n",
        "            print(f\"    Could not determine ARIMA order with auto_arima for {ticker}: {arima_e}\")\n",
        "            print(\"    Defaulting to (5,1,0) for ARIMA. Consider manually inspecting ACF/PACF plots.\")\n",
        "            arima_order = (5, 1, 0)\n",
        "\n",
        "        all_prepared_data['arima'][ticker] = {\n",
        "            'train_series': arima_series_train,\n",
        "            'test_series': arima_series_test,\n",
        "            'order': arima_order,\n",
        "            'scaler_y': scaler_y\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {ticker}: {e}\")\n",
        "\n",
        "try:\n",
        "    processed_file_path = os.path.join(processed_dir, 'all_prepared_stock_data.pkl')\n",
        "    with open(processed_file_path, 'wb') as f:\n",
        "        pickle.dump(all_prepared_data, f)\n",
        "    print(f\"\\nAll prepared data saved as a pickle file to: {processed_file_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving all_prepared_data: {e}\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "DC_KsZ3ofq04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de0c2b39-210d-43ff-8a84-a017bacaf389"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training temporary XGBoost model on AAPL to get feature importances...\n",
            "\n",
            "  Top 10 Features by Importance:\n",
            "          Feature  Importance\n",
            "30  Lag_BBM_5_2.0    0.601825\n",
            "4         SMA_200    0.167547\n",
            "31  Lag_BBU_5_2.0    0.087977\n",
            "1             Low    0.057373\n",
            "0            High    0.036961\n",
            "14      BBU_5_2.0    0.029919\n",
            "2            Open    0.008398\n",
            "29  Lag_BBL_5_2.0    0.003489\n",
            "5          SMA_50    0.002464\n",
            "13      BBM_5_2.0    0.001876\n",
            "\n",
            "  Selected Top 3 Features for all models: ['Lag_BBM_5_2.0', 'SMA_200', 'Lag_BBU_5_2.0']\n",
            "Cleaned data shape: X=(2313, 3), y=(2313,)\n",
            "Train/Test split: Train X=(1813, 3), y=(1813,) | Test X=(500, 3), y=(500,)\n",
            "    LSTM Train X shape: (1753, 60, 3), y shape: (1753, 1)\n",
            "    LSTM Test X shape: (440, 60, 3), y shape: (440, 1)\n",
            "    XGBoost Train X shape: (1813, 3), y shape: (1813,)\n",
            "    XGBoost Test X shape: (500, 3), y shape: (500,)\n",
            "    Auto-ARIMA found order (p,d,q): (0, 1, 1)\n",
            "Cleaned data shape: X=(2313, 3), y=(2313,)\n",
            "Train/Test split: Train X=(1813, 3), y=(1813,) | Test X=(500, 3), y=(500,)\n",
            "    LSTM Train X shape: (1753, 60, 3), y shape: (1753, 1)\n",
            "    LSTM Test X shape: (440, 60, 3), y shape: (440, 1)\n",
            "    XGBoost Train X shape: (1813, 3), y shape: (1813,)\n",
            "    XGBoost Test X shape: (500, 3), y shape: (500,)\n",
            "    Auto-ARIMA found order (p,d,q): (0, 1, 1)\n",
            "Cleaned data shape: X=(2313, 3), y=(2313,)\n",
            "Train/Test split: Train X=(1813, 3), y=(1813,) | Test X=(500, 3), y=(500,)\n",
            "    LSTM Train X shape: (1753, 60, 3), y shape: (1753, 1)\n",
            "    LSTM Test X shape: (440, 60, 3), y shape: (440, 1)\n",
            "    XGBoost Train X shape: (1813, 3), y shape: (1813,)\n",
            "    XGBoost Test X shape: (500, 3), y shape: (500,)\n",
            "    Auto-ARIMA found order (p,d,q): (0, 1, 0)\n",
            "Cleaned data shape: X=(2313, 3), y=(2313,)\n",
            "Train/Test split: Train X=(1813, 3), y=(1813,) | Test X=(500, 3), y=(500,)\n",
            "    LSTM Train X shape: (1753, 60, 3), y shape: (1753, 1)\n",
            "    LSTM Test X shape: (440, 60, 3), y shape: (440, 1)\n",
            "    XGBoost Train X shape: (1813, 3), y shape: (1813,)\n",
            "    XGBoost Test X shape: (500, 3), y shape: (500,)\n",
            "    Auto-ARIMA found order (p,d,q): (1, 1, 1)\n",
            "Cleaned data shape: X=(2313, 3), y=(2313,)\n",
            "Train/Test split: Train X=(1813, 3), y=(1813,) | Test X=(500, 3), y=(500,)\n",
            "    LSTM Train X shape: (1753, 60, 3), y shape: (1753, 1)\n",
            "    LSTM Test X shape: (440, 60, 3), y shape: (440, 1)\n",
            "    XGBoost Train X shape: (1813, 3), y shape: (1813,)\n",
            "    XGBoost Test X shape: (500, 3), y shape: (500,)\n",
            "    Auto-ARIMA found order (p,d,q): (1, 1, 0)\n",
            "Cleaned data shape: X=(2313, 3), y=(2313,)\n",
            "Train/Test split: Train X=(1813, 3), y=(1813,) | Test X=(500, 3), y=(500,)\n",
            "    LSTM Train X shape: (1753, 60, 3), y shape: (1753, 1)\n",
            "    LSTM Test X shape: (440, 60, 3), y shape: (440, 1)\n",
            "    XGBoost Train X shape: (1813, 3), y shape: (1813,)\n",
            "    XGBoost Test X shape: (500, 3), y shape: (500,)\n",
            "    Auto-ARIMA found order (p,d,q): (0, 1, 0)\n",
            "Cleaned data shape: X=(2313, 3), y=(2313,)\n",
            "Train/Test split: Train X=(1813, 3), y=(1813,) | Test X=(500, 3), y=(500,)\n",
            "    LSTM Train X shape: (1753, 60, 3), y shape: (1753, 1)\n",
            "    LSTM Test X shape: (440, 60, 3), y shape: (440, 1)\n",
            "    XGBoost Train X shape: (1813, 3), y shape: (1813,)\n",
            "    XGBoost Test X shape: (500, 3), y shape: (500,)\n",
            "    Auto-ARIMA found order (p,d,q): (5, 1, 4)\n",
            "Cleaned data shape: X=(2313, 3), y=(2313,)\n",
            "Train/Test split: Train X=(1813, 3), y=(1813,) | Test X=(500, 3), y=(500,)\n",
            "    LSTM Train X shape: (1753, 60, 3), y shape: (1753, 1)\n",
            "    LSTM Test X shape: (440, 60, 3), y shape: (440, 1)\n",
            "    XGBoost Train X shape: (1813, 3), y shape: (1813,)\n",
            "    XGBoost Test X shape: (500, 3), y shape: (500,)\n",
            "    Auto-ARIMA found order (p,d,q): (1, 1, 1)\n",
            "Cleaned data shape: X=(2313, 3), y=(2313,)\n",
            "Train/Test split: Train X=(1813, 3), y=(1813,) | Test X=(500, 3), y=(500,)\n",
            "    LSTM Train X shape: (1753, 60, 3), y shape: (1753, 1)\n",
            "    LSTM Test X shape: (440, 60, 3), y shape: (440, 1)\n",
            "    XGBoost Train X shape: (1813, 3), y shape: (1813,)\n",
            "    XGBoost Test X shape: (500, 3), y shape: (500,)\n",
            "    Auto-ARIMA found order (p,d,q): (2, 1, 0)\n",
            "Cleaned data shape: X=(2313, 3), y=(2313,)\n",
            "Train/Test split: Train X=(1813, 3), y=(1813,) | Test X=(500, 3), y=(500,)\n",
            "    LSTM Train X shape: (1753, 60, 3), y shape: (1753, 1)\n",
            "    LSTM Test X shape: (440, 60, 3), y shape: (440, 1)\n",
            "    XGBoost Train X shape: (1813, 3), y shape: (1813,)\n",
            "    XGBoost Test X shape: (500, 3), y shape: (500,)\n",
            "    Auto-ARIMA found order (p,d,q): (0, 1, 0)\n",
            "Cleaned data shape: X=(2313, 3), y=(2313,)\n",
            "Train/Test split: Train X=(1813, 3), y=(1813,) | Test X=(500, 3), y=(500,)\n",
            "    LSTM Train X shape: (1753, 60, 3), y shape: (1753, 1)\n",
            "    LSTM Test X shape: (440, 60, 3), y shape: (440, 1)\n",
            "    XGBoost Train X shape: (1813, 3), y shape: (1813,)\n",
            "    XGBoost Test X shape: (500, 3), y shape: (500,)\n",
            "    Auto-ARIMA found order (p,d,q): (0, 1, 0)\n",
            "Cleaned data shape: X=(2313, 3), y=(2313,)\n",
            "Train/Test split: Train X=(1813, 3), y=(1813,) | Test X=(500, 3), y=(500,)\n",
            "    LSTM Train X shape: (1753, 60, 3), y shape: (1753, 1)\n",
            "    LSTM Test X shape: (440, 60, 3), y shape: (440, 1)\n",
            "    XGBoost Train X shape: (1813, 3), y shape: (1813,)\n",
            "    XGBoost Test X shape: (500, 3), y shape: (500,)\n",
            "    Auto-ARIMA found order (p,d,q): (3, 1, 2)\n",
            "\n",
            "All prepared data saved as a pickle file to: /content/drive/MyDrive/Colab Notebooks/Processed Data/all_prepared_stock_data.pkl\n"
          ]
        }
      ]
    }
  ]
}